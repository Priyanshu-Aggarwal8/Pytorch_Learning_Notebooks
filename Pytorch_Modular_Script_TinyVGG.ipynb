{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOk9QYcJXtod6vJOmMOQwUW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanshu-Aggarwal8/Pytorch_Learning_Notebooks/blob/main/Pytorch_Modular_Script_TinyVGG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omr9YmPyAgSu"
      },
      "outputs": [],
      "source": [
        "# In this notebook, we will recreate the model we created in the previous notebook, but converting the model into executable python scripts along the way.\n",
        "# There are some Magic commands, like %%writefile, that are used to convert code to a python file.\n",
        "# By the end of this notebook, we will have a directory structure, using which we can quickly replicate our model and its functionalities inside another notebook as well as from the CLIs also."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"going_modular\", exist_ok = True)"
      ],
      "metadata": {
        "id": "ipyw4w4aC21-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory already exists\")\n",
        "else:\n",
        "    print(f\"Creating {image_path} directory\")\n",
        "    image_path.mkdir(parents = True, exist_ok = True)\n",
        "\n",
        "with open(data_path / \"pizza_steak_sushi.zip\" , \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "    print(\"Downloading pizza, steak, sushi data\")\n",
        "    f.write(request.content)\n",
        "\n",
        "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "  print(\"Unzipping data...\")\n",
        "  zip_ref.extractall(image_path)\n",
        "\n",
        "os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlZ7GtFwC-8m",
        "outputId": "977b9a42-acfd-41d2-8271-354c98fc0e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/pizza_steak_sushi directory already exists\n",
            "Downloading pizza, steak, sushi data\n",
            "Unzipping data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ],
      "metadata": {
        "id": "Gcs9Fin0D8Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder(root=train_dir,\n",
        "                                  transform=data_transform,\n",
        "                                  target_transform=None)\n",
        "\n",
        "test_data = datasets.ImageFolder(root=test_dir,\n",
        "                                 transform=data_transform)\n",
        "\n",
        "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JveVSlGLK9Cr",
        "outputId": "036ef862-e7a8-4af2-a628-55a80dfbae5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data:\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 225\n",
            "    Root location: data/pizza_steak_sushi/train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
            "               ToTensor()\n",
            "           )\n",
            "Test data:\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 75\n",
            "    Root location: data/pizza_steak_sushi/test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_dict = train_data.class_to_idx"
      ],
      "metadata": {
        "id": "yx99kRCdLGEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=1,\n",
        "                              num_workers=1,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=1,\n",
        "                             num_workers=1,\n",
        "                             shuffle=False)"
      ],
      "metadata": {
        "id": "IvDnvWh4LP2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = next(iter(train_dataloader))\n",
        "\n",
        "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
        "print(f\"Label shape: {label.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kktg9lgfLZZQ",
        "outputId": "b8feacd2-7967-4d8d-eb0c-238c2b553237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([1, 3, 64, 64]) -> [batch_size, color_channels, height, width]\n",
            "Label shape: torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating `data_setup.py`\n",
        "\n",
        "The above functionality can all be turned into a script and used everytime we want to load the data. We will now create a script named `data_setup.py` and within this script, create a function `create_dataloaders()`."
      ],
      "metadata": {
        "id": "YXcFhKvGFvu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/data_setup.py\n",
        "\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(train_dir : str,\n",
        "                       test_dir  :str,\n",
        "                       transform : transforms.Compose,\n",
        "                       batch_size : int,\n",
        "                       num_workers = NUM_WORKERS):\n",
        "    train_data = datasets.ImageFolder(train_dir, transform = transform)\n",
        "    test_data = datasets.ImageFolder(test_dir, transform = transform)\n",
        "\n",
        "    class_names = train_data.classes\n",
        "\n",
        "    train_dataloader = DataLoader(train_data,\n",
        "                                  shuffle = True,\n",
        "                                  batch_size = batch_size,\n",
        "                                  num_workers = num_workers,\n",
        "                                  pin_memory = True)\n",
        "    test_dataloader = DataLoader(test_data,\n",
        "                                 shuffle = False,\n",
        "                                 batch_size = batch_size,\n",
        "                                 num_workers = num_workers,\n",
        "                                 pin_memory = True)\n",
        "\n",
        "    return train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5zj6sbhFfoH",
        "outputId": "0f49e0db-5070-47b2-9115-4adf59ed64cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Creating `model_builder.py`"
      ],
      "metadata": {
        "id": "RGp_ITtZI5OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/model_builder.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  def __init__(self, input_shape : int, hidden_units : int, output_shape : int) -> None:\n",
        "    super().__init__()\n",
        "    self.conv_blvok_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = input_shape, out_channels = hidden_units, kernel_size = 3, stride = 1, padding = 0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels = hidden_units, out_channels = hidden_units, kernel_size = 3, stride = 1, padding = 0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "    )\n",
        "    self.conv_blvok_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = hidden_units, out_channels = hidden_units, kernel_size = 3, padding = 0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels = hidden_units, out_channels = hidden_units, kernel_size = 3, padding = 0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features = hidden_units * 13 * 13, out_features = output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x : torch.Tensor):\n",
        "    x = self.conv_blvok_1(x)\n",
        "    x = self.conv_blvok_2(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLxt1IWLHRsW",
        "outputId": "c7e40e22-6758-403e-8131-b19a95e7fb63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from going_modular import model_builder\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model_1 = model_builder.TinyVGG(input_shape = 3, hidden_units = 10, output_shape = 3).to(device)\n",
        "model_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q603vydSKGnS",
        "outputId": "2267ae19-410e-461a-d769-c51c5752f23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyVGG(\n",
              "  (conv_blvok_1): Sequential(\n",
              "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_blvok_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=1690, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_batch, label_batch = next(iter(train_dataloader))\n",
        "\n",
        "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
        "print(f\"Single image shape: {img_single.shape}\\n\")\n",
        "\n",
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "    pred = model_1(img_single.to(device))\n",
        "\n",
        "print(f\"Output logits:\\n{pred}\\n\")\n",
        "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
        "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
        "print(f\"Actual label:\\n{label_single}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDyMD1BmKbZG",
        "outputId": "64b5fc86-6fb5-4b08-c17a-c0d8a54bfc65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single image shape: torch.Size([1, 3, 64, 64])\n",
            "\n",
            "Output logits:\n",
            "tensor([[ 0.0208, -0.0020,  0.0095]], device='cuda:0')\n",
            "\n",
            "Output prediction probabilities:\n",
            "tensor([[0.3371, 0.3295, 0.3333]], device='cuda:0')\n",
            "\n",
            "Output prediction label:\n",
            "tensor([0], device='cuda:0')\n",
            "\n",
            "Actual label:\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating `engine.py`\n",
        "This script will contain the train_step(), test_step() and train() functions that we have previously created in other notebook."
      ],
      "metadata": {
        "id": "tGEJim4zLtBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/engine.py\n",
        "\n",
        "from typing import Dict, List, Tuple\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train_step(model : torch.nn.Module,\n",
        "               dataloader : torch.utils.data.DataLoader,\n",
        "               loss_fn : torch.nn.Module,\n",
        "               optimizer : torch.optim.Optimizer,\n",
        "               device : torch.device) -> Tuple[float, float]:\n",
        "\n",
        "              model.train()\n",
        "\n",
        "              train_loss, train_acc = 0, 0\n",
        "\n",
        "              for batch, (X, y) in enumerate(dataloader):\n",
        "                X, y = X.to(device) , y.to(device)\n",
        "\n",
        "                y_pred = model(X)\n",
        "\n",
        "                loss = loss_fn(y_pred, y)\n",
        "                train_loss += loss.item()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "                train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n",
        "\n",
        "              train_loss = train_loss / len(dataloader)\n",
        "              train_acc = train_acc / len(dataloader)\n",
        "              return train_loss, train_acc\n",
        "\n",
        "def test_step(model : torch.nn.Module,\n",
        "              dataloader : torch.utils.data.DataLoader,\n",
        "              loss_fn : torch.nn.Module,\n",
        "              device : torch.device) -> Tuple[float, float]:\n",
        "\n",
        "              model.eval()\n",
        "\n",
        "              test_loss, test_acc = 0, 0\n",
        "\n",
        "              with torch.inference_mode():\n",
        "                for batch, (X, y) in enumerate(dataloader):\n",
        "                  X, y = X.to(device), y.to(device)\n",
        "\n",
        "                  test_pred_logits = model(X)\n",
        "\n",
        "                  loss = loss_fn(test_pred_logits, y)\n",
        "                  test_loss += loss.item()\n",
        "\n",
        "                  test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "                  test_acc += ((test_pred_labels == y).sum().item() / len(test_pred_labels))\n",
        "\n",
        "              test_loss = test_loss / len(dataloader)\n",
        "              test_acc = test_acc / len(dataloader)\n",
        "              return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List[float]]:\n",
        "\n",
        "          results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "                }\n",
        "\n",
        "          for epoch in tqdm(range(epochs)):\n",
        "              train_loss, train_acc = train_step(model=model,\n",
        "                                                dataloader=train_dataloader,\n",
        "                                                loss_fn=loss_fn,\n",
        "                                                optimizer=optimizer,\n",
        "                                                device=device)\n",
        "              test_loss, test_acc = test_step(model=model,\n",
        "                                              dataloader=test_dataloader,\n",
        "                                              loss_fn=loss_fn,\n",
        "                                              device=device)\n",
        "\n",
        "              print(\n",
        "                f\"Epoch: {epoch+1} | \"\n",
        "                f\"train_loss: {train_loss:.4f} | \"\n",
        "                f\"train_acc: {train_acc:.4f} | \"\n",
        "                f\"test_loss: {test_loss:.4f} | \"\n",
        "                f\"test_acc: {test_acc:.4f}\"\n",
        "              )\n",
        "\n",
        "              results[\"train_loss\"].append(train_loss)\n",
        "              results[\"train_acc\"].append(train_acc)\n",
        "              results[\"test_loss\"].append(test_loss)\n",
        "              results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "          return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPGQwvMfL9Lp",
        "outputId": "233c0ec1-a996-4e98-912b-ecab00fba25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating `save_model()`"
      ],
      "metadata": {
        "id": "xTXkGd4FPXPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/utils.py\n",
        "\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "def save_model(model : torch.nn.Module,\n",
        "               target_dir : str,\n",
        "               model_name : str):\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents = True, exist_ok = True)\n",
        "\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOUhtRkfPT74",
        "outputId": "91258d8c-a18f-4378-c1ef-863eafa69c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating `train.py`"
      ],
      "metadata": {
        "id": "Sd9jcTxLQAPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/train.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import data_setup, engine, model_builder, utils\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "HIDDEN_UNITS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "train_dir = \"data/pizza_steak_sushi/train\"\n",
        "test_dir = \"data/pizza_steak_sushi/test\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "  transforms.Resize((64, 64)),\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=data_transform,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "model = model_builder.TinyVGG(\n",
        "    input_shape=3,\n",
        "    hidden_units=HIDDEN_UNITS,\n",
        "    output_shape=len(class_names)\n",
        ").to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=LEARNING_RATE)\n",
        "\n",
        "engine.train(model=model,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             epochs=NUM_EPOCHS,\n",
        "             device=device)\n",
        "\n",
        "utils.save_model(model=model,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK5vhTrIP4ig",
        "outputId": "05093ff0-96b7-4a05-d221-7ac1f3b9f028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python going_modular/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34woWIPvQkVh",
        "outputId": "fc8ae717-0eec-4ada-ad24-42674397ed38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0% 0/5 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.0968 | train_acc: 0.3672 | test_loss: 1.0934 | test_acc: 0.2604\n",
            " 20% 1/5 [00:01<00:04,  1.15s/it]Epoch: 2 | train_loss: 1.1134 | train_acc: 0.3047 | test_loss: 1.1396 | test_acc: 0.2604\n",
            " 40% 2/5 [00:02<00:02,  1.01it/s]Epoch: 3 | train_loss: 1.0895 | train_acc: 0.3125 | test_loss: 1.1227 | test_acc: 0.3125\n",
            " 60% 3/5 [00:02<00:01,  1.06it/s]Epoch: 4 | train_loss: 1.0798 | train_acc: 0.4336 | test_loss: 1.1276 | test_acc: 0.2083\n",
            " 80% 4/5 [00:03<00:00,  1.08it/s]Epoch: 5 | train_loss: 1.0453 | train_acc: 0.4141 | test_loss: 1.1238 | test_acc: 0.2083\n",
            "100% 5/5 [00:05<00:00,  1.05s/it]\n",
            "[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AB6vPxzOQrsI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}